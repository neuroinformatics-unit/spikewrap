{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Preprocessing Sessions\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This is a long-form tutorial on session preprocessing. See `here <preprocess-session-howto>` for a quick how-to.</p></div>\n\nIn this how-to we will use the :class:`spikewrap.Session` interface to manage\nthe preprocessing and visualisation of a dataset.\n\nWe will cover:\n    - Loading raw data for a session.\n    - Prototyping / visualising preprocessing steps.\n    - Saving preprocessed data.\n\n.. attention::\n\n   ``spikewrap``'s features are currently limited. See the `Roadmap <roadmap>`\n   for planned features.\n\nUnder the hood, spikewrap uses SpikeInterface to perform all preprocessing steps.\nSee the `Supported Preprocessing Steps <supported-preprocessing-tutorial>` for details\non supported functionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data\n\nTo load a session data, we must instantiate :class:`spikewrap.Session` object,\nwith the location of the data.\n\nThe examples in this tutorial examples use a project in\n[NeuroBlueprint](https://neuroblueprint.neuroinformatics.dev/latest/index.html) format,\n(however, some custom formats are supported, see `Supported Formats <supported-formats>` for details).\n\nLet's say we have a dataset (available at :class:`spikewrap.get_example_data_path()`) like:\n\n.. tab-set::\n    :sync-group: category\n\n    .. tab-item:: SpikeGLX\n\n```\n\u2514\u2500\u2500 rawdata/\n    \u2514\u2500\u2500 sub-001/\n        \u2514\u2500\u2500 ses-001/\n            \u2514\u2500\u2500 ephys/\n                \u251c\u2500\u2500 run-001_g0_imec0/\n                \u2502   \u251c\u2500\u2500 run-001_g0_t0.imec0.ap.bin\n                \u2502   \u2514\u2500\u2500 run-001_g0_t0.imec0.ap.meta\n                \u2514\u2500\u2500 run-002_g0_imec0/\n                    \u251c\u2500\u2500 run-002_g0_t0.imec0.ap.bin\n                    \u2514\u2500\u2500 run-002_g0_t0.imec0.ap.meta\n```\n    .. tab-item:: OpenEphys\n\n```\n\u2514\u2500\u2500 rawdata/\n    \u2514\u2500\u2500 sub-001/\n        \u2514\u2500\u2500 ses-001/\n            \u2514\u2500\u2500 ephys/\n                \u2514\u2500\u2500 Recording Node 304/\n                    \u2514\u2500\u2500 experiment1/\n                        \u251c\u2500\u2500 recording1/\n                        \u2502   \u2514\u2500\u2500 ...\n                        \u2514\u2500\u2500 recording2/\n                            \u2514\u2500\u2500 ...\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This dataset is installed with ``spikewrap`` so you can run this tutorial locally.\n\nFirst, we import and instantiate the :class:`spikewrap.Session` object with the:\n\n``subject_path``:\n   Full filepath to the subject folder in which the session is located.\n``session name``:\n   Name of the session folder.\n``file_format``:\n   ``\"spikeglx\"`` or ``\"openephys\"``, the acquisition software used.\n``run_names``:\n   (optional) Default \"``all\"`` or a list of run folder names to process.\n``probe``:\n   (optional) Default ``None``. Neuropixels are auto-detected from recording output.\n   Otherwise, a [ProbeInterface](https://probeinterface.readthedocs.io/en/main/)\n   object. This probe will be set on all runs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import spikewrap as sw\n\nsession = sw.Session(\n    subject_path=sw.get_example_data_path() / \"rawdata\" / \"sub-001\",\n    session_name=\"ses-001\",\n    file_format=\"spikeglx\",  # or \"openephys\"\n    run_names=\"all\"\n)\n\nsession.preprocess(configs=\"neuropixels+kilosort2_5\", concat_runs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to the magic of SpikeInterface, data loading and most preprocessing functions\nare 'lazy' and will be very fast. Note that **nothing is written to disk at this stage**.\n\nWe can inspect the detected run names with:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(session.get_raw_run_names())\n\n# and the names of the preprocessed runs (which may change to \"concat_run\"\n# if the runs are concatenated prior to preprocessing:\nprint(session.get_preprocessed_run_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing Options\n\nDefining preprocessing steps can be done in two ways, with a pre-set\nconfiguration file or a python dictionar (see `Managing Configs <configs-tutorial>` for more details).\n\nBriefly, a configuration file related to your data can be used to preprocess by name, as we did above.\n\nWe can print the configs used with:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sw.show_configs(\"neuropixels+kilosort2_5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Otherwise, we can define a dictionary with the steps to pass to :class:`spikewrap.Session.preprocess()`.\nPreprocess steps generally take the underlying SpikeInterface function name and parameters, see\n`Supported Preprocessing Steps <supported-preprocessing-tutorial>` for details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "configs = {\n    \"preprocessing\": {\n        \"1\": [\"phase_shift\", {}],\n        \"2\": [\"bandpass_filter\", {\"freq_min\": 300, \"freq_max\": 6000}],\n        \"3\": [\"common_reference\", {\"operator\": \"median\"}],\n    }\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`spikewrap.Session.preprocess()` will also accept a dictionary with the top-level omitted\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pp_steps = {\n    \"1\": [\"phase_shift\", {}],\n    \"2\": [\"bandpass_filter\", {\"freq_min\": 300, \"freq_max\": 6000}],\n    \"3\": [\"common_reference\", {\"operator\": \"median\"}],\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Concatenation Arguments\n\nFor multi-run sessions or multi-shank probes, we can set:\n\n``per_shank``:\n    If ``True``, split the recordings into separate shanks before preprocessing.\n``concat runs``:\n    If ``True``, concatenate all runs together before processing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "session.preprocess(\n    configs=configs,\n    per_shank=True,\n    concat_runs=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualising Preprocessing\n\n``spikewrap`` can be used to iteratively prototype preprocessing steps by adjusting\nconfigurations and arguments, then re-plotting. This can be performed in a\n[Jupyter notebook](https://jupyter.org/) if desired.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plots = session.plot_preprocessed(\n    show=True,\n    time_range=(0, 0.5),\n    show_channel_ids=False,  # also, \"mode\"=\"map\" or \"line\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``plots`` (a ``dict`` of matplotlib figures) contains the figures for (optional) further editing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(plots)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's update a preprocessing step and plot again:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\n\npp_attempt_2 = copy.deepcopy(configs)\n\n\n# This is currently quite verbose. It is the second preprocessing\n# step, second element of the list [\"function_name\", {function_kwargs...}]\n# (see processing dictionary defined above)\npp_attempt_2[\"preprocessing\"][\"3\"][1][\"operator\"] = \"average\"\n\nsession.preprocess(\n    configs=pp_attempt_2,\n    per_shank=False,\n    concat_runs=False,\n)\n\nplots = session.plot_preprocessed(\n    time_range=(0, 0.5), show_channel_ids=False, show=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Preprocessing\n\nWhen you are ready to save the preprocessed recording with your\nchosen settings, you can run:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "session.save_preprocessed(overwrite=True, n_jobs=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. attention::\n\n   On some systems, you may encounter strange behaviour when running multiple\n   jobs (``n_jobs > 1``), such as non-parallelised steps running more than once.\n\n   You may need to wrap your script  in a ``if __name__  == \"__main__\"`` block,\n   (if you encounter this problem, you will see an error to this effect).\n\n```\nif __name__ == \"__main__\":\n\n    import spikewrap as sw\n    ...\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using SLURM\n\nThe :class:`spikewrap.Session.save_preprocessed` step is where all data\nis preprocessed and written to disk. Therefore, if using a HPC (high-performance computing) system,\nit may be convenient to run it through the job-scheduler SLURM.\n\nThe function takes an argument ``slurm=True`` which can be used to save the preprocessing\nin a SLURM sbatch job.\n\nSee the `SLURM tutorial <slurm-tutorial>` for more information.\n\n.. attention::\n    SLURM jobs are requested at the **run level**. For example, if a\n    session has 2 runs (which are not concatenated),\n    :func:`spikewrap.Session.save_preprocessed` will request two nodes.\n\n\n### Output data organisation\n\nSee the\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}